{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate df object\n",
    "# ---------------------------------------------------------------------------------#\n",
    "def isDFThrow(df: pd.DataFrame):\n",
    "    \"\"\"Check if the input is a Pandas DataFrame, if not, raise ValueError.\"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"df is not a DataFrame\")\n",
    "\n",
    "\n",
    "# Move DF columns\n",
    "# ---------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def move_columns_to_end_by_len(df: pd.DataFrame, num_columns: int) -> pd.DataFrame:\n",
    "    # Revisar si el número de columnas a mover es válido\n",
    "    if num_columns >= len(df.columns):\n",
    "        print(\n",
    "            \"El número de columnas a mover excede el número total de columnas en el DataFrame.\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    # Reordenar las columnas\n",
    "    cols = list(df.columns)\n",
    "    cols = cols[num_columns:] + cols[:num_columns]\n",
    "    df = df[cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Comparar igualdad de decimales midiendo la precision decimal\n",
    "# ---------------------------------------------------------------------------------#\n",
    "def media_precision_decimales_by_index(df1, df2, indices):\n",
    "    precision_decimales = []\n",
    "\n",
    "    # Iterar sobre los índices proporcionados\n",
    "    for idx in indices:\n",
    "        # Obtener las filas correspondientes en los DataFrames\n",
    "        fila1 = df1.loc[idx]\n",
    "        fila2 = df2.loc[idx]\n",
    "\n",
    "        # Iterar sobre las columnas numéricas\n",
    "        for col in df1.select_dtypes(include=[\"float64\"]).columns:\n",
    "            # Comprobar si los valores de los decimales son iguales\n",
    "            if fila1[col] == fila2[col]:\n",
    "                # Calcular la precisión de los decimales\n",
    "                precision_decimales.append(len(str(fila1[col]).split(\".\")[1]))\n",
    "\n",
    "    # Calcular la media de la precisión de los decimales\n",
    "    if precision_decimales:\n",
    "        media = sum(precision_decimales) / len(precision_decimales)\n",
    "    else:\n",
    "        media = 0\n",
    "\n",
    "    return media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estratificado de datos\n",
    "\n",
    "- [ejecución paso a paso Code](/task/pia04-task/functions/splitDataToTrain.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDataToTrain(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        isDFThrow(df)\n",
    "\n",
    "        ###  split data to train\n",
    "        ## split data to train\n",
    "        X = df.drop(columns=\"median_house_value\")\n",
    "        y = df[\"median_house_value\"]\n",
    "\n",
    "        ### Muestreo estratificado (*Stratified sampling*)\n",
    "        ### dividiendo el *dataset* en grupos llamados **estratos**,\n",
    "        ### y asegurándose de tomar no solo un porcentaje de muestras del total,\n",
    "        ### sino ese porcentaje de cada estrato.\n",
    "        stratify = pd.cut(\n",
    "            df[\"median_income\"],\n",
    "            bins=[\n",
    "                0.0,\n",
    "                1.5,\n",
    "                3.0,\n",
    "                4.5,\n",
    "                6.0,\n",
    "                np.inf,\n",
    "            ],  # Secuencia de límites de los contenedores\n",
    "            labels=[1, 2, 3, 4, 5],  # dividimos en 5 categorías\n",
    "        )\n",
    "\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "        ## objetivo --> evitar el **sobre-ajuste** o ***over-fitting***\n",
    "        ##  X_train, X_test, y_train, y_test = train_test_split(\n",
    "        return train_test_split(\n",
    "            X,  # features\n",
    "            y,  # target\n",
    "            stratify=stratify,  # estratificado\n",
    "            test_size=0.2,  #  estoy usando el 20% de los datos # Controla la mezcla aplicada a los datos antes de aplicar la división.\n",
    "            random_state=42,  ## fixed seed\n",
    "        )\n",
    "    except ValueError as ve:\n",
    "        # Manejo de la excepción\n",
    "        print(\"ValueError:\", ve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución Secuencial\n",
    "\n",
    "- [ejecución paso a paso NoteBook](/task/pia04-task/pia04-task-sequential.ipynb)\n",
    "- [ejecución paso a paso Code](/task/pia04-task/functions/firstOheSequential.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstOheSequential(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        isDFThrow(df)\n",
    "        # dfVerifiqueNullAndNotNumberValues(df)\n",
    "        # columnsWithNonNumericValues = dfGetColumnsWithNonNumericValues(df)\n",
    "        # columnsWithNullValues = dfGetColumnsWithNullValues(df)\n",
    "        ## para comprobar la efectividad de las operaciones que vamos a realizar\n",
    "        ### obtenemos los índices de las filas con valores nulos\n",
    "        null_rows_idx = df.isnull().any(axis=1)\n",
    "\n",
    "        ## OHE\n",
    "        ### Aplicamos también OneHotEncoder para las variables categóricas.\n",
    "        cat_encoder = OneHotEncoder(sparse_output=False).set_output(\n",
    "            transform=\"pandas\"\n",
    "        )  # forzamos que la salida sea DataFrame\n",
    "        X_train_ocean_proximity_ohe = cat_encoder.fit_transform(df[[\"ocean_proximity\"]])\n",
    "\n",
    "        ### recuperar el resto de DataFrame\n",
    "        X_train_rest = df.drop(columns=\"ocean_proximity\")\n",
    "        data_cat_ohe = pd.concat([X_train_rest, X_train_ocean_proximity_ohe], axis=1)\n",
    "\n",
    "        ## Estandarización\n",
    "        scaler = StandardScaler().set_output(\n",
    "            transform=\"pandas\"\n",
    "        )  # Para que el resultado sea un DataFrame\n",
    "        X_train_scaled_and_ohe = scaler.fit_transform(data_cat_ohe)\n",
    "        # p(\"X_train_scaled_and_ohe\", X_train_scaled_and_ohe)\n",
    "\n",
    "        k_value = np.sqrt(df.shape[0]).astype(int)\n",
    "\n",
    "        ## Imputamos valores\n",
    "        X_train_imputed_a = (\n",
    "            KNNImputer(n_neighbors=k_value)\n",
    "            .set_output(transform=\"pandas\")\n",
    "            .fit_transform(X_train_scaled_and_ohe)\n",
    "        )\n",
    "\n",
    "        ## Verificamos la actualización de los valores nulos\n",
    "        X_train_imputed_a.loc[\n",
    "            null_rows_idx\n",
    "        ].head()  # visualizamos las filas que tenían valores nulos\n",
    "\n",
    "        return pd.DataFrame(X_train_imputed_a)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        # Manejo de la excepción\n",
    "        print(\"ValueError:\", ve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución mediante PipineLine\n",
    "\n",
    "- [ejecución paso a paso NoteBook](/task/pia04-task/pia04-task-pipe.ipynb)\n",
    "- [ejecución paso a paso Code](/task/pia04-task/functions/firstOhePipe.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstOhePipe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        # comprobaciones\n",
    "        isDFThrow(df)\n",
    "        # definimos variables\n",
    "        k_value = int(np.sqrt(df.shape[0]))\n",
    "        # Identificar las columnas categóricas y numéricas\n",
    "        categorical_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "        numerical_columns = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "        # Crear un Pipeline\n",
    "        ##  1. ejecute OHE - OneHotEncoder\n",
    "        ###  - dtype_include=\"object\" --> aplicamos solamente a datos no numericos\n",
    "        ###  -  remainder=\"passthrough\" --> tras aplicar los cambios necesitamos recuperar el resto de columnas\n",
    "        ##  2. estandarice - StandardScaler\n",
    "        ##  3. impute los datos nulos o faltantes - KNNImputer\n",
    "        pipeline = make_pipeline(\n",
    "            make_column_transformer(\n",
    "                (OneHotEncoder(), make_column_selector(dtype_include=\"object\")),\n",
    "                remainder=\"passthrough\",\n",
    "            ),\n",
    "            StandardScaler(),\n",
    "            KNNImputer(n_neighbors=k_value),\n",
    "        )\n",
    "        # Aplicamos las trasformaciones y las imputaciones al DataFrame\n",
    "        array_transformed = pipeline.fit_transform(df)\n",
    "        # Convertir el resultado imputado de nuevo a un DataFrame\n",
    "        ## Necesitamos obtener los nombres de las nuevas columnas después de OHE y el resto\n",
    "        ohe_columns = (\n",
    "            pipeline.named_steps[\"columntransformer\"]\n",
    "            .named_transformers_[\"onehotencoder\"]\n",
    "            .get_feature_names_out(input_features=categorical_columns)\n",
    "        )\n",
    "        all_columns = np.concatenate([ohe_columns, numerical_columns])\n",
    "        # Generamos un nuevo DF\n",
    "        df_transformed = pd.DataFrame(array_transformed, columns=all_columns)\n",
    "        # nota las columnas ohe se colocan al principio del df, las ponemos al final\n",
    "        # para que coincidan con eñ resultado secuencial\n",
    "        return move_columns_to_end_by_len(df_transformed, len(ohe_columns))\n",
    "\n",
    "    except ValueError as ve:\n",
    "        # Manejo de la excepción\n",
    "        print(\"ValueError:\", ve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparación de los resultados\n",
    "\n",
    "- [ejecución paso a paso NoteBook](/task/pia04-task/pia04-task-compare.ipynb)\n",
    "- [ejecución paso a paso Code](/task/pia04-task/functions/dfCompare.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFCompare(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    ") -> dict:\n",
    "    res = {}\n",
    "\n",
    "    # ❗Important to compare first reset index\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    res[\"df_size_eq\"] = df1.shape == df2.shape\n",
    "    res[\"df_data_type_eq\"] = df1.dtypes.equals(df2.dtypes)\n",
    "    res[\"df_columns_eq\"] = df1.columns.equals(df2.columns)\n",
    "    res[\"df_rows_eq\"] = df1.index.equals(df2.index)\n",
    "    res[\"df_value_00_eq\"] = (df1.iloc[0, 0]) == (df2.iloc[0, 0])\n",
    "    res[\"df_value_00_eq_round8\"] = (df1.iloc[0, 0].round(8)) == (\n",
    "        df2.iloc[0, 0].round(8)\n",
    "    )\n",
    "    res[\"df_values_eq\"] = (df1 == df2).all().all()\n",
    "    res[\"df_values_eq_round8\"] = (df1.round(8) == df2.round(8)).all().all()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main ejecución del Programa\n",
    "\n",
    "- [ejecución paso a paso Code](/task/pia04-task/main.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == Step1 Split Data to Train ==\n",
      " == Step2 OHE sequential ==\n",
      "                               12655     15502     2908 \n",
      "ocean_proximity_<1H OCEAN  -0.887683 -0.887683 -0.887683\n",
      "ocean_proximity_INLAND      1.462180 -0.683910  1.462180\n",
      "ocean_proximity_ISLAND     -0.011006 -0.011006 -0.011006\n",
      "ocean_proximity_NEAR BAY   -0.354889 -0.354889 -0.354889\n",
      "ocean_proximity_NEAR OCEAN -0.384217  2.602693 -0.384217\n",
      " == Step3 OHE pipe ==\n",
      "                                   0         1         2\n",
      "ocean_proximity_<1H OCEAN  -0.887683 -0.887683 -0.887683\n",
      "ocean_proximity_INLAND      1.462180 -0.683910  1.462180\n",
      "ocean_proximity_ISLAND     -0.011006 -0.011006 -0.011006\n",
      "ocean_proximity_NEAR BAY   -0.354889 -0.354889 -0.354889\n",
      "ocean_proximity_NEAR OCEAN -0.384217  2.602693 -0.384217\n",
      " == Step4 Compare OHE sequential vs OHE pipe ==\n",
      "df_size_eq : True\n",
      "df_data_type_eq : True\n",
      "df_columns_eq : True\n",
      "df_rows_eq : True\n",
      "df_value_00_eq : False\n",
      "df_value_00_eq_round8 : True\n",
      "df_values_eq : False\n",
      "df_values_eq_round8 : True\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/housing.csv\")\n",
    "\n",
    "print(\" == Step1 Split Data to Train ==\")\n",
    "X_train, X_test, y_train, y_test = SplitDataToTrain(df)\n",
    "null_rows_idx = df.isnull().any(axis=1)\n",
    "print(\" == Step2 OHE sequential ==\")\n",
    "ohe_sequential = FirstOheSequential(X_train.copy())\n",
    "print(ohe_sequential[ohe_sequential.columns[-5:]].head(3).T)\n",
    "print(\" == Step3 OHE pipe ==\")\n",
    "ohe_pipe = FirstOhePipe(X_train.copy())\n",
    "print(ohe_pipe[ohe_pipe.columns[-5:]].head(3).T)\n",
    "# onehotencoder_column_index = [1,2,3,4]\n",
    "print(\" == Step4 Compare OHE sequential vs OHE pipe ==\")\n",
    "compare = DFCompare(ohe_sequential, ohe_pipe)\n",
    "for key, value in compare.items():\n",
    "    print(key, \":\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "dt {\n",
    "  color:blue;\n",
    "  font-weight: bold;\n",
    "}\n",
    "dt {\n",
    "\n",
    "}\n",
    "</style>\n",
    "<dl>\n",
    "<dt>train_test_split</dt>\n",
    "<dd>This is the definition of the first term.</dd>\n",
    "<dt>OneHotEncoder</dt>\n",
    "<dd>This is one definition of the second term. </dd>\n",
    "<dt>StandardScaler</dt>\n",
    "<dd>This is one definition of the second term. </dd>\n",
    "<dt>KNNImputer(n_neighbors=k_value)</dt>\n",
    "<dd>This is one definition of the second term. </dd>\n",
    "<dt>make_pipeline</dt>\n",
    "<dd>This is one definition of the second term. </dd>\n",
    "<dt>make_column_transformer</dt>\n",
    "<dd>This is one definition of the second term. </dd>\n",
    "<dd>atributo: remainder=\"passthrough\"</dd>\n",
    "</dl>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
